{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8e06a3e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pptx import Presentation\n",
    "from bs4 import BeautifulSoup as bs\n",
    "import markdown\n",
    "import os\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1db31e6",
   "metadata": {},
   "source": [
    "### Read in Obsidian files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "e61debeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ProfileBuilder():\n",
    "    def __init__(self):\n",
    "        print(\"init()\")\n",
    "        \n",
    "        self.path = None\n",
    "        self.entities_to_parse = []\n",
    "        self.raw_entity_data = {}\n",
    "        self.final_entity_data = {}\n",
    "        \n",
    "        self.PERSON_ATTRIBUTES = (\n",
    "            (\"Detail\", \"details\"),\n",
    "            (\"Note\", \"notes\"),\n",
    "            (\"Associate\", \"associates\"),\n",
    "            (\"Email Address\", \"email_addresses\"),\n",
    "            (\"Phone Number\", \"phone_numbers\"),\n",
    "            (\"Residence Name\", \"residence_names\"),\n",
    "            (\"Residence Address\", \"residence_addresses\"),\n",
    "            (\"Residence Coordinates\", \"residence_coordinates\"),\n",
    "            (\"Residence Map\", \"residence_maps\"),\n",
    "            (\"Work Name\", \"work_names\"),\n",
    "            (\"Work Address\", \"work_addresses\"),\n",
    "            (\"Work Coordinates\", \"work_coordinates\"),\n",
    "            (\"Work Map\", \"work_maps\"),\n",
    "        )\n",
    "        \n",
    "    def create_profiles(self, path: str, entity_type: str):\n",
    "        print(\"create_profiles()\")\n",
    "        self.path = path\n",
    "        \n",
    "        self.scan_dir_for_md_files()\n",
    "        \n",
    "        self.read_md_files_into_soup(entity_type)\n",
    "        print(len(self.raw_entity_data))\n",
    "        \n",
    "        self.parse_entity_info_from_soup()\n",
    "    \n",
    "    def scan_dir_for_md_files(self):\n",
    "#         entities_to_parse = []\n",
    "        for file in os.listdir(self.path):\n",
    "            if file.endswith(\"md\"):\n",
    "                self.entities_to_parse.append(file)\n",
    "#         return entities_to_parse\n",
    "    \n",
    "    def read_md_files_into_soup(self, entity_type):\n",
    "        for i in self.entities_to_parse[0:1]:\n",
    "            name = i.replace(\".md\", \"\")\n",
    "            self.raw_entity_data[name] = {\"entity_type\": entity_type}\n",
    "            self.final_entity_data[name] = {\"entity_type\": entity_type}\n",
    "\n",
    "            html = None\n",
    "            with open(f\"{self.path}/{i}\", \"r\", encoding=\"utf-8\") as f:\n",
    "                text = f.read()\n",
    "\n",
    "            html = markdown.markdown(text)\n",
    "            soup = bs(html)\n",
    "            self.raw_entity_data[name][\"soup\"] = soup\n",
    "        \n",
    "    def parse_entity_info_from_soup(self):\n",
    "        for name, data in self.raw_entity_data.items():\n",
    "            \n",
    "            # Change to switch statement !\n",
    "            if data[\"entity_type\"] == \"person\":\n",
    "                print(\"this is a person profile\")\n",
    "                self.parse_person_info(name, data[\"soup\"])\n",
    "            elif data[\"entity_type\"] == \"place\":\n",
    "                pass\n",
    "            else:\n",
    "                pass\n",
    "    \n",
    "    def parse_person_info(self, name, soup):\n",
    "        # Get all paragraph and list item tags\n",
    "        all_p_and_li_tags = []\n",
    "        all_p_and_li_tags += soup.find_all(\"p\")\n",
    "        all_p_and_li_tags += soup.find_all(\"li\")\n",
    "#         print(all_p_and_li_tags)\n",
    "        \n",
    "        # Find pic filename and store pic path\n",
    "        pic_path = self.get_person_pic_path(all_p_and_li_tags)\n",
    "        self.final_entity_data[name][\"person_pic_path\"] = pic_path        \n",
    "\n",
    "        self.final_entity_data[name].update(self.find_tags_matching_attributes(all_p_and_li_tags))\n",
    "#         print(self.final_entity_data)\n",
    "\n",
    "\n",
    "    def get_person_pic_path(self, tags):\n",
    "        person_pic = None\n",
    "\n",
    "        # Find correct p tag to parse\n",
    "        try:\n",
    "            person_pic = list(filter(lambda x: \"Picture\" in x.get_text(), tags))[0]\n",
    "        except IndexError:\n",
    "            print(\"There is no p or li tag with a picture in it\")\n",
    "            pass\n",
    "\n",
    "        if not person_pic:\n",
    "            return\n",
    "\n",
    "        # Parse filename from p tag\n",
    "        bracketed_filename_pattern = \"([0-9A-Z]{1,}[\\_\\s]{0,}[0-9A-Z]{1,}\\.{1}[0-9A-Z]{1,3})\"\n",
    "        try:\n",
    "            person_pic = re.search(bracketed_filename_pattern, str(person_pic), re.IGNORECASE)[0]\n",
    "        except TypeError:\n",
    "            print(\"There is no pic filename\")\n",
    "            person_pic = None\n",
    "            pass\n",
    "        \n",
    "        if not person_pic:\n",
    "            return\n",
    "        \n",
    "        return f\"Assets/{person_pic.strip()}\"\n",
    "    \n",
    "    def find_tags_matching_attributes(self, tags):\n",
    "        info = {}\n",
    "        \n",
    "        for attr in self.PERSON_ATTRIBUTES:\n",
    "#             print(\"attr: \", attr)\n",
    "            info[attr[1]] = []\n",
    "            \n",
    "            matching_tags = list(filter(lambda x: attr[0] in x.get_text(), tags))\n",
    "#             print(\"matching tags: \", matching_tags)\n",
    "            \n",
    "            cleaned_texts = self.extract_info_from_matching_tags(matching_tags)\n",
    "            info[attr[1]] += cleaned_texts\n",
    "        \n",
    "        return info\n",
    "    \n",
    "    def extract_info_from_matching_tags(self, tags):\n",
    "#         print(f\"There are {len(tags)} matching tags here\")\n",
    "        tags = self.remove_code_tag_substring(tags)\n",
    "        tags = self.get_text_inside_tags(tags)\n",
    "        tags = self.extract_bracketed_link_texts(tags)\n",
    "        return tags\n",
    "        \n",
    "    def remove_code_tag_substring(self, tags):\n",
    "        cleaned_tags = []\n",
    "        for tag in tags:\n",
    "            cleaned_tag = None\n",
    "            try:\n",
    "                cleaned_tag = re.sub(\"<code>.*</code>\", \"\", str(tag))\n",
    "            except:\n",
    "                # print(\"substring doesn't have a `Code` tag in it\")\n",
    "                pass\n",
    "            if cleaned_tag:\n",
    "                cleaned_tags.append(cleaned_tag)\n",
    "        return cleaned_tags\n",
    "\n",
    "    def get_text_inside_tags(self, tags):\n",
    "        inside_texts = []\n",
    "        for tag in tags:            \n",
    "            inside_text = None\n",
    "            # Check for text inside p tags\n",
    "            try:\n",
    "                inside_text = re.match(\"(<p>)(.*)(</p>)\", tag)[2]\n",
    "            except TypeError:\n",
    "                # print(\"No text inside p tags\")\n",
    "                pass\n",
    "            # Check for text inside li tags\n",
    "            try:\n",
    "                inside_text = re.match(\"(<li>)(.*)(</li>)\", tag)[2]\n",
    "            except TypeError:\n",
    "                # print(\"No text inside li tags\")\n",
    "                pass\n",
    "            if inside_text:\n",
    "                inside_texts.append(inside_text.strip())\n",
    "        return inside_texts\n",
    "\n",
    "    def extract_bracketed_link_texts(self, texts):\n",
    "        cleaned_texts = []\n",
    "        for text in texts:\n",
    "            cleaned_text = None\n",
    "            try:\n",
    "                match_obj = re.match(\"(.*)(\\[\\[)(.*)(\\]\\])(.*)\", text)        \n",
    "        #         print(\"0\", match_obj[0]) # match object\n",
    "        #         print(\"1\", match_obj[1]) # match group 1, etc...\n",
    "        #         print(\"2\", match_obj[2])\n",
    "        #         print(\"3\", match_obj[3])\n",
    "        #         print(\"4\", match_obj[4])\n",
    "        #         print(\"5\", match_obj[5])\n",
    "                text_before = match_obj[1]\n",
    "                text_inside = match_obj[3]\n",
    "                text_after = match_obj[5]\n",
    "                rebuilt = f\"{text_before.strip()} {text_inside.strip()} {text_after.strip()}\"\n",
    "                cleaned_text = rebuilt.strip()\n",
    "            except TypeError:\n",
    "                # print(\"There was no linked text in double brackets\")\n",
    "                cleaned_text = text.strip()\n",
    "            if cleaned_text:\n",
    "                cleaned_texts.append(cleaned_text)\n",
    "        return cleaned_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "5296b850",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "init()\n",
      "create_profiles()\n",
      "1\n",
      "this is a person profile\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'Arron Gaines': {'entity_type': 'person',\n",
       "  'person_pic_path': 'Assets/Arron_Gaines.jpg',\n",
       "  'details': ['Subject likely lives and works in New York City, New York based on social media including pictures and check-ins. Follower/following relationships on social media platforms indicate a connection with Austin, Texas-based Kevin Ellison . Research on social engineering forums revealed usernames that were traced back to email addresses with a common web domain which has been separately flagged for malicious activity. Additional research on the emails suggests Gaines and Ellison are the probable end users.'],\n",
       "  'notes': ['No previous criminal convictions',\n",
       "   'Known phishing victims include: [[Elsie Graves]], [[Wilfredo Goodman]], and Jan Jackson'],\n",
       "  'associates': ['Kevin Ellison'],\n",
       "  'email_addresses': ['<em>probably uses</em> mglanman@mycbt(.)me'],\n",
       "  'phone_numbers': ['+1472-217-7096', '+1(202)-918-2132'],\n",
       "  'residence_names': ['350 Central Park West Apartments'],\n",
       "  'residence_addresses': ['350 Central Park West, New York, NY 10025'],\n",
       "  'residence_coordinates': ['40.79078819123121, -73.96568568759587'],\n",
       "  'residence_maps': ['<br/> ! 350_Central_Park_West_Apartments.JPG | 500'],\n",
       "  'work_names': ['undetermined organization at 30 Rockefeller Plaza'],\n",
       "  'work_addresses': ['30 Rockefeller Plaza, New York, NY 10112'],\n",
       "  'work_coordinates': ['40.758941635700246, -73.97900684718394'],\n",
       "  'work_maps': ['<br/> ! 30_Rockefeller_Plaza.JPG | 500']}}"
      ]
     },
     "execution_count": 147,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_dir = \"Investigation/People\"\n",
    "\n",
    "builder = ProfileBuilder()\n",
    "builder.create_profiles(target_dir, \"person\")\n",
    "\n",
    "builder.final_entity_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d9968e",
   "metadata": {},
   "source": [
    "- location maps should use similar logic to person pic path to get just the file name (regex)\n",
    "- try re.sub again all tags to get rid of things like <em></em>\n",
    "- need to unpack names from double brackets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "daa4edb7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# # Determine if file is for Person of Interest\n",
    "# to_find = \"Person_Of_Interest\"\n",
    "\n",
    "# is_person_file = any([to_find in tag.get_text() for tag in all_p_tags])\n",
    "# is_person_file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
